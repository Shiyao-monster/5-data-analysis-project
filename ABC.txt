
import ray
from ray.util.actor_pool import ActorPool

ray.init()

# Step 1: Create a placement group with a spread strategy and 5 CPUs per bundle
placement_group = ray.util.placement_group(
    [{"CPU": 5} for _ in range(6)], strategy="SPREAD"
)

# Wait until the placement group is ready
ray.get(placement_group.ready())

# Step 2: Define your actor class
@ray.remote
class TaskProcessor:
    def process(self, data):
        # Replace with your actual processing logic
        return f"Processed {data}"

# Step 3: Create actors within the placement group
actors = [
    TaskProcessor.options(placement_group=placement_group).remote() for _ in range(30)
]

# Step 4: Initialize an ActorPool with the created actors
actor_pool = ActorPool(actors)

# Step 5: Use the ActorPool to map tasks across the actors
tasks = list(range(30))  # Example tasks

# Submit tasks to the pool
results = actor_pool.map(lambda a, v: a.process.remote(v), tasks)

# Collect results
for result in results:
    print(result)





import re

def sum_time_values(text):
    # Regular expression to match "I/O batch {batch_number} time: {time_value}"
    time_pattern = r'I/O batch \d+ time: (\d+(\.\d+)?)'
    
    # Find all time values using regex
    time_values = re.findall(time_pattern, text)
    
    # Sum up the time values
    total_time = sum(float(time_value[0]) for time_value in time_values)
    
    return total_time

# Example usage
massive_text = """
    Some log text I/O batch 1 time: 2.34 more log text
    Another line I/O batch 2 time: 5.67 additional info
    Yet another log I/O batch 3 time: 1.23 end of line
"""
total_time = sum_time_values(massive_text)
print(f"Total time: {total_time} seconds")






import sys.process._
import org.apache.spark.sql.SparkSession

// Initialize the Spark session
val spark = SparkSession.builder.appName("SharedMemoryCheck").getOrCreate()

// Function to retrieve /dev/shm info
def getSharedMemoryInfo(): String = {
  val shmInfo = "df -h /dev/shm".!!.trim // Run the shell command to get shared memory info
  shmInfo
}

// Create an RDD with as many partitions as you have worker nodes
val rdd = spark.sparkContext.parallelize(1 to spark.sparkContext.defaultParallelism, spark.sparkContext.defaultParallelism)

// Run the shared memory check on each worker node
val shmInfoRDD = rdd.mapPartitions(_ => Iterator(getSharedMemoryInfo())).collect()

// Display shared memory info for each worker
shmInfoRDD.zipWithIndex.foreach { case (info, idx) =>
  println(s"Worker Node $idx Shared Memory Info:\n$info")
}



import spark.implicits._
import scala.collection.JavaConverters._
import sys.process._

// Function to execute a shell command to get /dev/shm info
def getSharedMemoryInfo(): String = {
  val shmInfo = "df -h /dev/shm".!!.trim
  shmInfo
}

// Create a DataFrame to distribute the task to all worker nodes
val procs = (1 to 1000).toDF().map(_ => {
  val hostname = "hostname".!!.trim
  val cpus = Runtime.getRuntime.availableProcessors
  val memory = getSharedMemoryInfo()
  (hostname, cpus, memory)
}).collectAsList().asScala.toMap

// Print shared memory info for each worker node
procs.foreach { case (hostname, (cpus, memory)) =>
  println(s"Hostname: $hostname")
  println(s"CPUs: $cpus")
  println(s"Shared Memory Info: $memory")
}









import java.lang.management.ManagementFactory
import com.sun.management.OperatingSystemMXBean
import spark.implicits._
import scala.collection.JavaConverters._
import sys.process._

val procs = (1 to 1000).toDF().map(_ => {
  val hostname = "hostname".!!.trim
  val cpus = Runtime.getRuntime.availableProcessors
  val osBean = ManagementFactory.getPlatformMXBean(classOf[OperatingSystemMXBean])
  val memory = osBean.getTotalPhysicalMemorySize() / (1024 * 1024) // Memory in MB
  (hostname, cpus, memory)
}).collectAsList().asScala.toMap

val workerNodeCpus = procs.values.map(_._2).sum
val workerNodeMemory = procs.values.map(_._3).sum

println(s"Worker Node CPUs: $workerNodeCpus")
println(s"Worker Node Memory: $workerNodeMemory MB")






from pyspark.sql import SparkSession
import os

# Initialize Spark session
spark = SparkSession.builder.appName("SharedMemoryCheck").getOrCreate()

# Function to execute on worker nodes
def check_shared_memory(_):
    shm_info = os.popen("df -h /dev/shm").read()
    yield shm_info

# Create an RDD to force worker execution (the number of partitions should correspond to your worker nodes)
rdd = spark.sparkContext.parallelize(range(spark.sparkContext.defaultParallelism), spark.sparkContext.defaultParallelism)

# Run the shared memory check on each partition (worker node)
shm_info_rdd = rdd.mapPartitions(check_shared_memory).collect()

# Print the shared memory info for each worker node
for idx, shm_info in enumerate(shm_info_rdd):
    print(f"Worker {idx} shared memory info:\n{shm_info}")



import ray
import asyncio
import time

# Initialize Ray
ray.init()

class ClassA:
    def __init__(self, num_batches=500):
        self.num_batches = num_batches

    # IO-bound task (asynchronous)
    async def io_bound_task(self, batch_id):
        print(f"Starting IO-bound task for batch {batch_id}")
        await asyncio.sleep(1)  # Simulate an async IO task
        print(f"Completed IO-bound task for batch {batch_id}")
        return batch_id

    # CPU-bound task (synchronous, to be run in Ray actor)
    def cpu_bound_task(self, batch_id):
        print(f"Starting CPU-bound task for batch {batch_id}")
        time.sleep(1)  # Simulate a CPU-bound computation task
        print(f"Completed CPU-bound task for batch {batch_id}")
        return f"Batch {batch_id} processed"

    # Actor to handle CPU-bound tasks
    @ray.remote
    class CPUWorker:
        def process(self, batch_id):
            return ClassA().cpu_bound_task(batch_id)

    # Main function to process batches
    async def process_batches(self):
        # Create one IO-bound worker and seven CPU-bound workers
        cpu_workers = [self.CPUWorker.remote() for _ in range(7)]

        for batch_id in range(self.num_batches):
            # Step 1: Handle IO-bound task asynchronously
            batch_id = await self.io_bound_task(batch_id)

            # Step 2: Handle CPU-bound task using available CPU worker
            available_worker = cpu_workers[batch_id % 7]  # Select worker in a round-robin fashion
            result = await available_worker.process.remote(batch_id)

            print(ray.get(result))  # Get the result from CPU worker

# Example of running the code
if __name__ == "__main__":
    class_a = ClassA()
    asyncio.run(class_a.process_batches())




FROM databricksruntime/standard:14.3

RUN apt-get update && apt-get install -y \
    software-properties-common \
    dirmngr \
    gnupg2 \
    curl \
    build-essential \
    libssl-dev \
    zlib1g-dev \
    libbz2-dev \
    libreadline-dev \
    libsqlite3-dev \
    wget \
    libncursesw5-dev \
    libgdbm-dev \
    libc6-dev \
    liblzma-dev \
    libffi-dev \
    uuid-dev \
    libxml2-dev

RUN apt-key adv --keyserver keyserver.ubuntu.com --recv-keys E084DAB9 \
    && add-apt-repository "deb https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/"

RUN apt-get update && apt-get install -y r-base=3.6.3-1bionic

RUN add-apt-repository ppa:deadsnakes/ppa \
    && apt-get update \
    && apt-get install -y python3.8 python3.8-dev python3.8-venv

RUN update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.8 1 \
    && update-alternatives --config python3

RUN curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py \
    && python3 get-pip.py \
    && rm get-pip.py

RUN apt-get clean && rm -rf /var/lib/apt/lists/*

RUN R --version && python3 --version && pip --version
